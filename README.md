# NeuralPoet
Poetry sampled from a Recurrent Neural Network language model which was trained on a web-scraped dataset of 261 poems

### Sample poem generated by the network:
i never write<br>
for not hot or cold again<br>
backwards of death by the other mistake we had<br>
the smallest artists sit outside into the first<br>
rocks steam and the women cry<br>
in one night the trees without death again<br>
i held work and one by one who not beside it for him with<br>
or so much that still be today here<br>


Try it for yourself!


### Project setup
Install dependencies with `pip install -r requirements.txt`<br>
Run `predict.py` to produce a novel sample from the language model. This sample will use weights that were pre-trained on an AWS EC2 p2.xlarge instance for 5 hours.<br>
Run `train.py` to further train the weights on your own computer.


# How it works

### Recurrent Neural Network (RNN)
The dataset of poems `poems.txt` is divided into sequences. The network is trained to take a sequence of words as input and predict the next words. Recurrent neurons differ from regular neurons because they are able to take sequences as input.

The long-short term memory (LSTM) recurrent unit was specifically used in this network because of its capability to find long-term dependencies. It does this by training specific weights, called gates, which allow the unit to store information between inputs. These gates determine which data is important to store long-term (like the gender of a subject) and which data should be removed (like the previously stored gender, once a new subject is introduced in the text).  

*Note*: tests using the Gated Recurrent Unit (GRU) produced similar results)

### Incorporating Word2Vec
Words are typically one-hot-encoded or encoded with integer indeces for input into neural networks. When humans read text they already have a language model in their brains which provides them with contextual information about each word. However, these methods treat each word like an individual entity, and the networks are forced to learn the contextual information themselves (such as how China and Beijing refer to similar places). Unfortunately, the network isn't even being trained to do this task

Encoding words as word-embeddings solves this issue. Word embeddings are high-dimensional vectors which represent the meaning of a word. Distances between words' embeddings indicate their level of similarity. Embeddings are generated through a dedicated supervised learning task. Although the `Word2VecModel` class in `rap_models.py` does have functionality to train a Word2Vec model on the 261 poems `poems.txt`, the weights in `model_weights.h5` were trained with a model that used pre-trained embeddings from online.

### Which pre-trained word embeddings did you use?
I used Google's collection of 3 million vectors that were pretrained on a Google News corpus which contained billions of words. You can find it here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit

### Why use a Keras embedding layer?
The corpus used to train the model consisted of over 20k words. Storing the individual Word2Vec embeddings for each word requires a lot of storage space and slows down training (since each word appears many times in the vocabulary). The embedding layer (the first layer in the network) takes in a word's index and outputs that word's Word2Vec embedding. This largly reduces the size of the training set, as it allows each high-dimensional vector to be replaced with a single integer ID.

### Handling infrequent words
Words that appear infrequently are encoded with the unknown word token (`<UKN>`). The network is simply provided this token, rather than the actual word itself. The logic behind this is that when a word is rare, the network should not waste energy learning its meaning. Furthermore, enabling the network to process unknown word allows  any english sentence to be used as a seed for the network, even if some of its words don't appear in the training set.

### Sampling the language model
The network is seeded with the zero vector. It then roduces a probability distribution indicating the likelihood that each word in the vocabularity is the next word in the sequence. This probability distribution is used to randomly select a word from the vocabulary. Each word that is generated is added to the sequence which is then fed back into the network. The sampling continues until the network produces the `<END>` token.

### Acknowledgements
This project was inspired by Andrej Karparthy's infamous blog post: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ 
Andrej trained an RNN to read text character-by-character and predict the next character that would appear. I decided to iterate on this method by building a network to processes text word-by-word. This allowed me to incoporate Word2Vec embeddings and produce more coherent sentances.

